{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day84.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c7CBy8vgPQy",
        "colab_type": "text"
      },
      "source": [
        "<!-- wp:paragraph -->\n",
        "<p>In the previous <a href=\"https://tekworld.org/2019/02/01/day-83-100-days-mlcode-conditional-adversarial-networks/\">blog</a>, we discussed the Conditional GAN and in the blog, we are going to discuss the CycleGAN.</p>\n",
        "<!-- /wp:paragraph -->\n",
        "\n",
        "<!-- wp:heading {\"level\":4} -->\n",
        "<h4>CycleGAN</h4>\n",
        "<!-- /wp:heading -->\n",
        "\n",
        "<!-- wp:paragraph -->\n",
        "<p>in 2017, the idea was published for <a href=\"https://arxiv.org/abs/1703.10593\">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks,</a> In <g class=\"gr_ gr_56 gr-alert gr_gramm gr_inline_cards gr_run_anim Grammar only-ins replaceWithoutSep\" id=\"56\" data-gr-id=\"56\">previous</g> blog, we discussed how Pixel2Pixel was able not only generate the new image but also keeping the original version of the image. In CycleGAN we are generating the image from other image but both do not have to be in a pair. Like in <g class=\"gr_ gr_445 gr-alert gr_gramm gr_inline_cards gr_run_anim Grammar only-ins replaceWithoutSep\" id=\"445\" data-gr-id=\"445\">previous</g> blog, if you draw a cat and pass it as input, based on the <g class=\"gr_ gr_388 gr-alert gr_spell gr_inline_cards gr_run_anim ContextualSpelling ins-del multiReplace\" id=\"388\" data-gr-id=\"388\">trianed</g> model it was able to generate an image of Cat. This was because input and train image <g class=\"gr_ gr_559 gr-alert gr_gramm gr_inline_cards gr_run_anim Grammar multiReplace\" id=\"559\" data-gr-id=\"559\">were</g> <g class=\"gr_ gr_558 gr-alert gr_gramm gr_inline_cards gr_run_anim Grammar only-ins doubleReplace replaceWithoutSep\" id=\"558\" data-gr-id=\"558\">pair</g> of images. However, for many primary objective, paired training data may not be available and there CycleGAN comes into picutre.</p>\n",
        "<!-- /wp:paragraph -->\n",
        "\n",
        "<!-- wp:paragraph -->\n",
        "<p>As per paper, </p>\n",
        "<!-- /wp:paragraph -->\n",
        "\n",
        "<!-- wp:quote -->\n",
        "<blockquote class=\"wp-block-quote\"><p>We present an approach for learning to translate an image from a source domain&nbsp;X&nbsp;to a target domain&nbsp;Y&nbsp;in the absence of paired examples. Our goal is to learn a mapping&nbsp;G:X→Y&nbsp;such that the distribution of images from&nbsp;G(X)&nbsp;is indistinguishable from the distribution&nbsp;Yusing an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping&nbsp;F:Y→X&nbsp;and introduce a cycle consistency loss to push&nbsp;F(G(X))≈X&nbsp;(and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.</p><cite>https://arxiv.org/pdf/1703.10593.pdf</cite></blockquote>\n",
        "<!-- /wp:quote -->\n",
        "\n",
        "<!-- wp:heading {\"level\":4} -->\n",
        "<h4>Unpaired Image to Image Translation:</h4>\n",
        "<!-- /wp:heading -->\n",
        "\n",
        "<!-- wp:paragraph -->\n",
        "<p>Below image from the paper confirms how the unpaired image get trained compared to paired image. Paired training data (left) consists of training examples {xi, yi} where i=1 to n, where the correspondence between xi<br> and yi exists. Right hand side is CycleGan an unpaired training<br> data (right), consisting of a source set {xi} i=1 to n (xi ∈ X)<br> and a target set {yj}j=1 to n (yj ∈ Y ), with no information provided as to which xi matches which yj .</p>\n",
        "<!-- /wp:paragraph -->\n",
        "\n",
        "<!-- wp:image {\"id\":616} -->\n",
        "<figure class=\"wp-block-image\"><img src=\"https://tekworld.org/wp-content/uploads/2019/02/Screen-Shot-2019-02-02-at-8.56.46-PM-1024x604.png\" alt=\"\" class=\"wp-image-616\"/><figcaption>Unpair Image to Image Translation ( source of the image: Original Paper)</figcaption></figure>\n",
        "<!-- /wp:image -->\n",
        "\n",
        "<!-- wp:paragraph -->\n",
        "<p>The main advantage of CycleGAN  in being able to learn the mapping between images without one-to-one mapping between training data in source and target domains. The requirement of paired image in the target domain is removed by making a two-step transformation of source domain image - first by trying to map it to target domain and then back to the original image. Mapping the image to target domain is done using a generator network and the quality of this generated image is improved by pitching the generator against a discriminator </p>\n",
        "<!-- /wp:paragraph -->\n",
        "\n",
        "<!-- wp:heading {\"level\":3} -->\n",
        "<h3>CycleGAN Architecture</h3>\n",
        "<!-- /wp:heading -->\n",
        "\n",
        "<!-- wp:paragraph -->\n",
        "<p>This network contains two stride-2 convolutions, several residual blocks, and two fractionallystrided convolutions with stride 1/2. It uses 6 blocks for<br> 128 × 128 images and 9 blocks for 256 × 256 and higher resolution training images. CycleGAN uses instance normalization. For the discriminator<br> networks it uses 70 × 70 PatchGANs, which aims to classify whether 70 × 70 overlapping image patches are real or fake. Such a patch-level discriminator architecture has fewer parameters than a full-image discriminator and can work on arbitrarily-sized images in a fully convolutional fashion</p>\n",
        "<!-- /wp:paragraph -->\n",
        "\n",
        "<!-- wp:image -->\n",
        "<figure class=\"wp-block-image\"><img src=\"https://hardikbansal.github.io/CycleGANBlog/images/model.jpg\" alt=\"Model Architecture\"/></figure>\n",
        "<!-- /wp:image -->\n",
        "\n",
        "<!-- wp:image -->\n",
        "<figure class=\"wp-block-image\"><img src=\"https://hardikbansal.github.io/CycleGANBlog/images/model1.jpg\" alt=\"Model Architecture 1\"/><figcaption>Architecture of the CycelGAN ( Both images are copied from the website https://hardikbansal.github.io/CycleGANBlog/)</figcaption></figure>\n",
        "<!-- /wp:image -->\n",
        "\n",
        "<!-- wp:paragraph -->\n",
        "<p>Above two images displays how the CycleGAN works . Two inputs are fed into each discriminator(one is original image corresponding to that domain and other is the generated image via a generator) and the aim of discriminator is to differentiate between them and reject images generated by generator. On other hand the generator would like to make sure that these images get accepted by the discriminator, so it will try to generate images which are very close to original images in Class. </p>\n",
        "<!-- /wp:paragraph -->\n",
        "\n",
        "<!-- wp:heading {\"level\":4} -->\n",
        "<h4>Implementation</h4>\n",
        "<!-- /wp:heading -->\n",
        "\n",
        "<!-- wp:paragraph -->\n",
        "<p>Original Implementation of the paper can found <a href=\"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\">here</a>. We are going to use TensorFlow instead of Pytorch. </p>\n",
        "<!-- /wp:paragraph -->\n",
        "\n",
        "<!-- wp:heading {\"level\":4} -->\n",
        "<h4>References</h4>\n",
        "<!-- /wp:heading -->\n",
        "\n",
        "<!-- wp:paragraph -->\n",
        "<p><a href=\"https://hardikbansal.github.io/CycleGANBlog/\">https://hardikbansal.github.io/CycleGANBlog/</a></p>\n",
        "<!-- /wp:paragraph -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WZX1p97iPNR",
        "colab_type": "text"
      },
      "source": [
        "Parameters for the Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_lAVfacnjpp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "492c2c4f-ca5e-459a-8c0b-68bf20cee918"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/hardikbansal/CycleGAN/master/download_datasets.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-03 05:40:25--  https://raw.githubusercontent.com/hardikbansal/CycleGAN/master/download_datasets.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 812 [text/plain]\n",
            "Saving to: ‘download_datasets.sh’\n",
            "\n",
            "\rdownload_datasets.s   0%[                    ]       0  --.-KB/s               \rdownload_datasets.s 100%[===================>]     812  --.-KB/s    in 0s      \n",
            "\n",
            "2019-02-03 05:40:26 (113 MB/s) - ‘download_datasets.sh’ saved [812/812]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx2AJFT3npXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "FILE='horse2zebra'\n",
        "\n",
        "if [[ $FILE != \"ae_photos\" && $FILE != \"apple2orange\" && $FILE != \"summer2winter_yosemite\" &&  $FILE != \"horse2zebra\" && $FILE != \"monet2photo\" && $FILE != \"cezanne2photo\" && $FILE != \"ukiyoe2photo\" && $FILE != \"vangogh2photo\" && $FILE != \"maps\" && $FILE != \"cityscapes\" && $FILE != \"facades\" && $FILE != \"iphone2dslr_flower\" && $FILE != \"ae_photos\" ]]; then\n",
        "    echo \"Available datasets are: apple2orange, summer2winter_yosemite, horse2zebra, monet2photo, cezanne2photo, ukiyoe2photo, vangogh2photo, maps, cityscapes, facades, iphone2dslr_flower, ae_photos\"\n",
        "    exit 1\n",
        "fi\n",
        "\n",
        "URL=https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip\n",
        "ZIP_FILE=./input/horse2zebra.zip\n",
        "TARGET_DIR=./input/horse2zebra/\n",
        "wget -N $URL -O ZIP_FILE\n",
        "mkdir TARGET_DIR\n",
        "unzip ZIP_FILE -d ./input/\n",
        "rm ZIP_FILE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcU03ssWk1TG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f3f7982c-7f65-434e-c623-909bd25a79f2"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/hardikbansal/CycleGAN/master/layers.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-03 05:41:14--  https://raw.githubusercontent.com/hardikbansal/CycleGAN/master/layers.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2533 (2.5K) [text/plain]\n",
            "Saving to: ‘layers.py’\n",
            "\n",
            "\rlayers.py             0%[                    ]       0  --.-KB/s               \rlayers.py           100%[===================>]   2.47K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-02-03 05:41:14 (52.1 MB/s) - ‘layers.py’ saved [2533/2533]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dR7cXbmk1GT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Basic Code is taken from https://github.com/ckmarkoh/GAN-tensorflow\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import numpy as np\n",
        "from scipy.misc import imsave\n",
        "import os\n",
        "import shutil\n",
        "from PIL import Image\n",
        "import time\n",
        "import random\n",
        "\n",
        "\n",
        "from layers import *\n",
        "\n",
        "img_height = 256\n",
        "img_width = 256\n",
        "img_layer = 3\n",
        "img_size = img_height * img_width\n",
        "\n",
        "\n",
        "batch_size = 1\n",
        "pool_size = 50\n",
        "ngf = 32\n",
        "ndf = 64\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def build_resnet_block(inputres, dim, name=\"resnet\"):\n",
        "    \n",
        "    with tf.variable_scope(name):\n",
        "\n",
        "        out_res = tf.pad(inputres, [[0, 0], [1, 1], [1, 1], [0, 0]], \"REFLECT\")\n",
        "        out_res = general_conv2d(out_res, dim, 3, 3, 1, 1, 0.02, \"VALID\",\"c1\")\n",
        "        out_res = tf.pad(out_res, [[0, 0], [1, 1], [1, 1], [0, 0]], \"REFLECT\")\n",
        "        out_res = general_conv2d(out_res, dim, 3, 3, 1, 1, 0.02, \"VALID\",\"c2\",do_relu=False)\n",
        "        \n",
        "        return tf.nn.relu(out_res + inputres)\n",
        "\n",
        "\n",
        "def build_generator_resnet_6blocks(inputgen, name=\"generator\"):\n",
        "    with tf.variable_scope(name):\n",
        "        f = 7\n",
        "        ks = 3\n",
        "        \n",
        "        pad_input = tf.pad(inputgen,[[0, 0], [ks, ks], [ks, ks], [0, 0]], \"REFLECT\")\n",
        "        o_c1 = general_conv2d(pad_input, ngf, f, f, 1, 1, 0.02,name=\"c1\")\n",
        "        o_c2 = general_conv2d(o_c1, ngf*2, ks, ks, 2, 2, 0.02,\"SAME\",\"c2\")\n",
        "        o_c3 = general_conv2d(o_c2, ngf*4, ks, ks, 2, 2, 0.02,\"SAME\",\"c3\")\n",
        "\n",
        "        o_r1 = build_resnet_block(o_c3, ngf*4, \"r1\")\n",
        "        o_r2 = build_resnet_block(o_r1, ngf*4, \"r2\")\n",
        "        o_r3 = build_resnet_block(o_r2, ngf*4, \"r3\")\n",
        "        o_r4 = build_resnet_block(o_r3, ngf*4, \"r4\")\n",
        "        o_r5 = build_resnet_block(o_r4, ngf*4, \"r5\")\n",
        "        o_r6 = build_resnet_block(o_r5, ngf*4, \"r6\")\n",
        "\n",
        "        o_c4 = general_deconv2d(o_r6, [batch_size,64,64,ngf*2], ngf*2, ks, ks, 2, 2, 0.02,\"SAME\",\"c4\")\n",
        "        o_c5 = general_deconv2d(o_c4, [batch_size,128,128,ngf], ngf, ks, ks, 2, 2, 0.02,\"SAME\",\"c5\")\n",
        "        o_c5_pad = tf.pad(o_c5,[[0, 0], [ks, ks], [ks, ks], [0, 0]], \"REFLECT\")\n",
        "        o_c6 = general_conv2d(o_c5_pad, img_layer, f, f, 1, 1, 0.02,\"VALID\",\"c6\",do_relu=False)\n",
        "\n",
        "        # Adding the tanh layer\n",
        "\n",
        "        out_gen = tf.nn.tanh(o_c6,\"t1\")\n",
        "\n",
        "\n",
        "        return out_gen\n",
        "\n",
        "def build_generator_resnet_9blocks(inputgen, name=\"generator\"):\n",
        "    with tf.variable_scope(name):\n",
        "        f = 7\n",
        "        ks = 3\n",
        "        \n",
        "        pad_input = tf.pad(inputgen,[[0, 0], [ks, ks], [ks, ks], [0, 0]], \"REFLECT\")\n",
        "        o_c1 = general_conv2d(pad_input, ngf, f, f, 1, 1, 0.02,name=\"c1\")\n",
        "        o_c2 = general_conv2d(o_c1, ngf*2, ks, ks, 2, 2, 0.02,\"SAME\",\"c2\")\n",
        "        o_c3 = general_conv2d(o_c2, ngf*4, ks, ks, 2, 2, 0.02,\"SAME\",\"c3\")\n",
        "\n",
        "        o_r1 = build_resnet_block(o_c3, ngf*4, \"r1\")\n",
        "        o_r2 = build_resnet_block(o_r1, ngf*4, \"r2\")\n",
        "        o_r3 = build_resnet_block(o_r2, ngf*4, \"r3\")\n",
        "        o_r4 = build_resnet_block(o_r3, ngf*4, \"r4\")\n",
        "        o_r5 = build_resnet_block(o_r4, ngf*4, \"r5\")\n",
        "        o_r6 = build_resnet_block(o_r5, ngf*4, \"r6\")\n",
        "        o_r7 = build_resnet_block(o_r6, ngf*4, \"r7\")\n",
        "        o_r8 = build_resnet_block(o_r7, ngf*4, \"r8\")\n",
        "        o_r9 = build_resnet_block(o_r8, ngf*4, \"r9\")\n",
        "\n",
        "        o_c4 = general_deconv2d(o_r9, [batch_size,128,128,ngf*2], ngf*2, ks, ks, 2, 2, 0.02,\"SAME\",\"c4\")\n",
        "        o_c5 = general_deconv2d(o_c4, [batch_size,256,256,ngf], ngf, ks, ks, 2, 2, 0.02,\"SAME\",\"c5\")\n",
        "        o_c6 = general_conv2d(o_c5, img_layer, f, f, 1, 1, 0.02,\"SAME\",\"c6\",do_relu=False)\n",
        "\n",
        "        # Adding the tanh layer\n",
        "\n",
        "        out_gen = tf.nn.tanh(o_c6,\"t1\")\n",
        "\n",
        "\n",
        "        return out_gen\n",
        "\n",
        "\n",
        "def build_gen_discriminator(inputdisc, name=\"discriminator\"):\n",
        "\n",
        "    with tf.variable_scope(name):\n",
        "        f = 4\n",
        "\n",
        "        o_c1 = general_conv2d(inputdisc, ndf, f, f, 2, 2, 0.02, \"SAME\", \"c1\", do_norm=False, relufactor=0.2)\n",
        "        o_c2 = general_conv2d(o_c1, ndf*2, f, f, 2, 2, 0.02, \"SAME\", \"c2\", relufactor=0.2)\n",
        "        o_c3 = general_conv2d(o_c2, ndf*4, f, f, 2, 2, 0.02, \"SAME\", \"c3\", relufactor=0.2)\n",
        "        o_c4 = general_conv2d(o_c3, ndf*8, f, f, 1, 1, 0.02, \"SAME\", \"c4\",relufactor=0.2)\n",
        "        o_c5 = general_conv2d(o_c4, 1, f, f, 1, 1, 0.02, \"SAME\", \"c5\",do_norm=False,do_relu=False)\n",
        "\n",
        "        return o_c5\n",
        "\n",
        "\n",
        "def patch_discriminator(inputdisc, name=\"discriminator\"):\n",
        "\n",
        "    with tf.variable_scope(name):\n",
        "        f= 4\n",
        "\n",
        "        patch_input = tf.random_crop(inputdisc,[1,70,70,3])\n",
        "        o_c1 = general_conv2d(patch_input, ndf, f, f, 2, 2, 0.02, \"SAME\", \"c1\", do_norm=\"False\", relufactor=0.2)\n",
        "        o_c2 = general_conv2d(o_c1, ndf*2, f, f, 2, 2, 0.02, \"SAME\", \"c2\", relufactor=0.2)\n",
        "        o_c3 = general_conv2d(o_c2, ndf*4, f, f, 2, 2, 0.02, \"SAME\", \"c3\", relufactor=0.2)\n",
        "        o_c4 = general_conv2d(o_c3, ndf*8, f, f, 2, 2, 0.02, \"SAME\", \"c4\", relufactor=0.2)\n",
        "        o_c5 = general_conv2d(o_c4, 1, f, f, 1, 1, 0.02, \"SAME\", \"c5\",do_norm=False,do_relu=False)\n",
        "\n",
        "        return o_c5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPH7BQYGgLS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ngf = 32 # Number of filters in first layer of generator\n",
        "ndf = 64 # Number of filters in first layer of discriminator\n",
        "batch_size = 1 # batch_size\n",
        "pool_size = 50 # pool_size\n",
        "img_width = 256 # Imput image will of width 256\n",
        "img_height = 256 # Input image will be of height 256\n",
        "img_depth = 3 # RGB format\n",
        "\n",
        "from layers import *\n",
        "#from model import *\n",
        "\n",
        "img_height = 256\n",
        "img_width = 256\n",
        "img_layer = 3\n",
        "img_size = img_height * img_width\n",
        "\n",
        "to_train = True\n",
        "to_test = False\n",
        "to_restore = False\n",
        "output_path = \"./output\"\n",
        "check_dir = \"./output/checkpoints/\"\n",
        "\n",
        "\n",
        "temp_check = 0\n",
        "\n",
        "\n",
        "\n",
        "max_epoch = 1\n",
        "max_images = 100\n",
        "\n",
        "h1_size = 150\n",
        "h2_size = 300\n",
        "z_size = 100\n",
        "batch_size = 1\n",
        "pool_size = 50\n",
        "sample_size = 10\n",
        "save_training_images = True\n",
        "ngf = 32\n",
        "ndf = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZPt_DXTiUxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CycleGAN():\n",
        "\n",
        "    def input_setup(self):\n",
        "\n",
        "        ''' \n",
        "        This function basically setup variables for taking image input.\n",
        "        filenames_A/filenames_B -> takes the list of all training images\n",
        "        self.image_A/self.image_B -> Input image with each values ranging from [-1,1]\n",
        "        '''\n",
        "\n",
        "        filenames_A = tf.train.match_filenames_once(\"./input/horse2zebra/trainA/*.jpg\")    \n",
        "        self.queue_length_A = tf.size(filenames_A)\n",
        "        filenames_B = tf.train.match_filenames_once(\"./input/horse2zebra/trainB/*.jpg\")    \n",
        "        self.queue_length_B = tf.size(filenames_B)\n",
        "        \n",
        "        filename_queue_A = tf.train.string_input_producer(filenames_A)\n",
        "        filename_queue_B = tf.train.string_input_producer(filenames_B)\n",
        "\n",
        "        image_reader = tf.WholeFileReader()\n",
        "        _, image_file_A = image_reader.read(filename_queue_A)\n",
        "        _, image_file_B = image_reader.read(filename_queue_B)\n",
        "        print(\"image_file_A\")\n",
        "        self.image_A = tf.subtract(tf.div(tf.image.resize_images(tf.image.decode_jpeg(image_file_A),[256,256]),127.5),1)\n",
        "        self.image_B = tf.subtract(tf.div(tf.image.resize_images(tf.image.decode_jpeg(image_file_B),[256,256]),127.5),1)\n",
        "\n",
        "    \n",
        "\n",
        "    def input_read(self, sess):\n",
        "\n",
        "\n",
        "        '''\n",
        "        It reads the input into from the image folder.\n",
        "        self.fake_images_A/self.fake_images_B -> List of generated images used for calculation of loss function of Discriminator\n",
        "        self.A_input/self.B_input -> Stores all the training images in python list\n",
        "        '''\n",
        "\n",
        "        # Loading images into the tensors\n",
        "        coord = tf.train.Coordinator()\n",
        "        threads = tf.train.start_queue_runners(coord=coord)\n",
        "        try:\n",
        "          num_files_A = sess.run(self.queue_length_A)\n",
        "        except:\n",
        "           exit\n",
        "        try:\n",
        "          num_files_B = sess.run(self.queue_length_B)\n",
        "        except:\n",
        "          exit\n",
        "\n",
        "        self.fake_images_A = np.zeros((pool_size,1,img_height, img_width, img_layer))\n",
        "        self.fake_images_B = np.zeros((pool_size,1,img_height, img_width, img_layer))\n",
        "\n",
        "\n",
        "        self.A_input = np.zeros((max_images, batch_size, img_height, img_width, img_layer))\n",
        "        self.B_input = np.zeros((max_images, batch_size, img_height, img_width, img_layer))\n",
        "\n",
        "        for i in range(max_images): \n",
        "            image_tensor = sess.run(self.image_A)\n",
        "            if(image_tensor.size() == img_size*batch_size*img_layer):\n",
        "                self.A_input[i] = image_tensor.reshape((batch_size,img_height, img_width, img_layer))\n",
        "\n",
        "        for i in range(max_images):\n",
        "            image_tensor = sess.run(self.image_B)\n",
        "            if(image_tensor.size() == img_size*batch_size*img_layer):\n",
        "                self.B_input[i] = image_tensor.reshape((batch_size,img_height, img_width, img_layer))\n",
        "\n",
        "\n",
        "        coord.request_stop()\n",
        "        coord.join(threads)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def model_setup(self):\n",
        "\n",
        "        ''' This function sets up the model to train\n",
        "        self.input_A/self.input_B -> Set of training images.\n",
        "        self.fake_A/self.fake_B -> Generated images by corresponding generator of input_A and input_B\n",
        "        self.lr -> Learning rate variable\n",
        "        self.cyc_A/ self.cyc_B -> Images generated after feeding self.fake_A/self.fake_B to corresponding generator. This is use to calcualte cyclic loss\n",
        "        '''\n",
        "\n",
        "        self.input_A = tf.placeholder(tf.float32, [batch_size, img_width, img_height, img_layer], name=\"input_A\")\n",
        "        self.input_B = tf.placeholder(tf.float32, [batch_size, img_width, img_height, img_layer], name=\"input_B\")\n",
        "        \n",
        "        self.fake_pool_A = tf.placeholder(tf.float32, [None, img_width, img_height, img_layer], name=\"fake_pool_A\")\n",
        "        self.fake_pool_B = tf.placeholder(tf.float32, [None, img_width, img_height, img_layer], name=\"fake_pool_B\")\n",
        "\n",
        "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "\n",
        "        self.num_fake_inputs = 0\n",
        "\n",
        "        self.lr = tf.placeholder(tf.float32, shape=[], name=\"lr\")\n",
        "\n",
        "        with tf.variable_scope(\"Model\") as scope:\n",
        "            self.fake_B = build_generator_resnet_9blocks(self.input_A, name=\"g_A\")\n",
        "            self.fake_A = build_generator_resnet_9blocks(self.input_B, name=\"g_B\")\n",
        "            self.rec_A = build_gen_discriminator(self.input_A, \"d_A\")\n",
        "            self.rec_B = build_gen_discriminator(self.input_B, \"d_B\")\n",
        "\n",
        "            scope.reuse_variables()\n",
        "\n",
        "            self.fake_rec_A = build_gen_discriminator(self.fake_A, \"d_A\")\n",
        "            self.fake_rec_B = build_gen_discriminator(self.fake_B, \"d_B\")\n",
        "            self.cyc_A = build_generator_resnet_9blocks(self.fake_B, \"g_B\")\n",
        "            self.cyc_B = build_generator_resnet_9blocks(self.fake_A, \"g_A\")\n",
        "\n",
        "            scope.reuse_variables()\n",
        "\n",
        "            self.fake_pool_rec_A = build_gen_discriminator(self.fake_pool_A, \"d_A\")\n",
        "            self.fake_pool_rec_B = build_gen_discriminator(self.fake_pool_B, \"d_B\")\n",
        "\n",
        "    def loss_calc(self):\n",
        "\n",
        "        ''' In this function we are defining the variables for loss calcultions and traning model\n",
        "        d_loss_A/d_loss_B -> loss for discriminator A/B\n",
        "        g_loss_A/g_loss_B -> loss for generator A/B\n",
        "        *_trainer -> Variaous trainer for above loss functions\n",
        "        *_summ -> Summary variables for above loss functions'''\n",
        "\n",
        "        cyc_loss = tf.reduce_mean(tf.abs(self.input_A-self.cyc_A)) + tf.reduce_mean(tf.abs(self.input_B-self.cyc_B))\n",
        "        \n",
        "        disc_loss_A = tf.reduce_mean(tf.squared_difference(self.fake_rec_A,1))\n",
        "        disc_loss_B = tf.reduce_mean(tf.squared_difference(self.fake_rec_B,1))\n",
        "        \n",
        "        g_loss_A = cyc_loss*10 + disc_loss_B\n",
        "        g_loss_B = cyc_loss*10 + disc_loss_A\n",
        "\n",
        "        d_loss_A = (tf.reduce_mean(tf.square(self.fake_pool_rec_A)) + tf.reduce_mean(tf.squared_difference(self.rec_A,1)))/2.0\n",
        "        d_loss_B = (tf.reduce_mean(tf.square(self.fake_pool_rec_B)) + tf.reduce_mean(tf.squared_difference(self.rec_B,1)))/2.0\n",
        "\n",
        "        \n",
        "        optimizer = tf.train.AdamOptimizer(self.lr, beta1=0.5)\n",
        "\n",
        "        self.model_vars = tf.trainable_variables()\n",
        "\n",
        "        d_A_vars = [var for var in self.model_vars if 'd_A' in var.name]\n",
        "        g_A_vars = [var for var in self.model_vars if 'g_A' in var.name]\n",
        "        d_B_vars = [var for var in self.model_vars if 'd_B' in var.name]\n",
        "        g_B_vars = [var for var in self.model_vars if 'g_B' in var.name]\n",
        "        \n",
        "        self.d_A_trainer = optimizer.minimize(d_loss_A, var_list=d_A_vars)\n",
        "        self.d_B_trainer = optimizer.minimize(d_loss_B, var_list=d_B_vars)\n",
        "        self.g_A_trainer = optimizer.minimize(g_loss_A, var_list=g_A_vars)\n",
        "        self.g_B_trainer = optimizer.minimize(g_loss_B, var_list=g_B_vars)\n",
        "\n",
        "        for var in self.model_vars: print(var.name)\n",
        "\n",
        "        #Summary variables for tensorboard\n",
        "\n",
        "        self.g_A_loss_summ = tf.summary.scalar(\"g_A_loss\", g_loss_A)\n",
        "        self.g_B_loss_summ = tf.summary.scalar(\"g_B_loss\", g_loss_B)\n",
        "        self.d_A_loss_summ = tf.summary.scalar(\"d_A_loss\", d_loss_A)\n",
        "        self.d_B_loss_summ = tf.summary.scalar(\"d_B_loss\", d_loss_B)\n",
        "\n",
        "    def save_training_images(self, sess, epoch):\n",
        "\n",
        "        if not os.path.exists(\"./output/imgs\"):\n",
        "            os.makedirs(\"./output/imgs\")\n",
        "\n",
        "        for i in range(0,10):\n",
        "            fake_A_temp, fake_B_temp, cyc_A_temp, cyc_B_temp = sess.run([self.fake_A, self.fake_B, self.cyc_A, self.cyc_B],feed_dict={self.input_A:self.A_input[i], self.input_B:self.B_input[i]})\n",
        "            imsave(\"./output/imgs/fakeB_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((fake_A_temp[0]+1)*127.5).astype(np.uint8))\n",
        "            imsave(\"./output/imgs/fakeA_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((fake_B_temp[0]+1)*127.5).astype(np.uint8))\n",
        "            imsave(\"./output/imgs/cycA_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((cyc_A_temp[0]+1)*127.5).astype(np.uint8))\n",
        "            imsave(\"./output/imgs/cycB_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((cyc_B_temp[0]+1)*127.5).astype(np.uint8))\n",
        "            imsave(\"./output/imgs/inputA_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((self.A_input[i][0]+1)*127.5).astype(np.uint8))\n",
        "            imsave(\"./output/imgs/inputB_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((self.B_input[i][0]+1)*127.5).astype(np.uint8))\n",
        "\n",
        "    def fake_image_pool(self, num_fakes, fake, fake_pool):\n",
        "        ''' This function saves the generated image to corresponding pool of images.\n",
        "        In starting. It keeps on feeling the pool till it is full and then randomly selects an\n",
        "        already stored image and replace it with new one.'''\n",
        "\n",
        "        if(num_fakes < pool_size):\n",
        "            fake_pool[num_fakes] = fake\n",
        "            return fake\n",
        "        else :\n",
        "            p = random.random()\n",
        "            if p > 0.5:\n",
        "                random_id = random.randint(0,pool_size-1)\n",
        "                temp = fake_pool[random_id]\n",
        "                fake_pool[random_id] = fake\n",
        "                return temp\n",
        "            else :\n",
        "                return fake\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "\n",
        "        ''' Training Function '''\n",
        "\n",
        "\n",
        "        # Load Dataset from the dataset folder\n",
        "        self.input_setup()  \n",
        "\n",
        "        #Build the network\n",
        "        self.model_setup()\n",
        "\n",
        "        #Loss function calculations\n",
        "        self.loss_calc()\n",
        "      \n",
        "        # Initializing the global variables\n",
        "        init = tf.global_variables_initializer()\n",
        "        saver = tf.train.Saver()     \n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(init)\n",
        "\n",
        "            #Read input to nd array\n",
        "            self.input_read(sess)\n",
        "\n",
        "            #Restore the model to run the model from last checkpoint\n",
        "            if to_restore:\n",
        "                chkpt_fname = tf.train.latest_checkpoint(check_dir)\n",
        "                saver.restore(sess, chkpt_fname)\n",
        "\n",
        "            writer = tf.summary.FileWriter(\"./output/2\")\n",
        "\n",
        "            if not os.path.exists(check_dir):\n",
        "                os.makedirs(check_dir)\n",
        "\n",
        "            # Training Loop\n",
        "            for epoch in range(sess.run(self.global_step),100):                \n",
        "                print (\"In the epoch \", epoch)\n",
        "                saver.save(sess,os.path.join(check_dir,\"cyclegan\"),global_step=epoch)\n",
        "\n",
        "                # Dealing with the learning rate as per the epoch number\n",
        "                if(epoch < 100) :\n",
        "                    curr_lr = 0.0002\n",
        "                else:\n",
        "                    curr_lr = 0.0002 - 0.0002*(epoch-100)/100\n",
        "\n",
        "                if(save_training_images):\n",
        "                    self.save_training_images(sess, epoch)\n",
        "\n",
        "                # sys.exit()\n",
        "\n",
        "                for ptr in range(0,max_images):\n",
        "                    print(\"In the iteration \",ptr)\n",
        "                    print(\"Starting\",time.time()*1000.0)\n",
        "\n",
        "                    # Optimizing the G_A network\n",
        "\n",
        "                    _, fake_B_temp, summary_str = sess.run([self.g_A_trainer, self.fake_B, self.g_A_loss_summ],feed_dict={self.input_A:self.A_input[ptr], self.input_B:self.B_input[ptr], self.lr:curr_lr})\n",
        "                    \n",
        "                    writer.add_summary(summary_str, epoch*max_images + ptr)                    \n",
        "                    fake_B_temp1 = self.fake_image_pool(self.num_fake_inputs, fake_B_temp, self.fake_images_B)\n",
        "                    \n",
        "                    # Optimizing the D_B network\n",
        "                    _, summary_str = sess.run([self.d_B_trainer, self.d_B_loss_summ],feed_dict={self.input_A:self.A_input[ptr], self.input_B:self.B_input[ptr], self.lr:curr_lr, self.fake_pool_B:fake_B_temp1})\n",
        "                    writer.add_summary(summary_str, epoch*max_images + ptr)\n",
        "                    \n",
        "                    \n",
        "                    # Optimizing the G_B network\n",
        "                    _, fake_A_temp, summary_str = sess.run([self.g_B_trainer, self.fake_A, self.g_B_loss_summ],feed_dict={self.input_A:self.A_input[ptr], self.input_B:self.B_input[ptr], self.lr:curr_lr})\n",
        "\n",
        "                    writer.add_summary(summary_str, epoch*max_images + ptr)\n",
        "                    \n",
        "                    \n",
        "                    fake_A_temp1 = self.fake_image_pool(self.num_fake_inputs, fake_A_temp, self.fake_images_A)\n",
        "\n",
        "                    # Optimizing the D_A network\n",
        "                    _, summary_str = sess.run([self.d_A_trainer, self.d_A_loss_summ],feed_dict={self.input_A:self.A_input[ptr], self.input_B:self.B_input[ptr], self.lr:curr_lr, self.fake_pool_A:fake_A_temp1})\n",
        "\n",
        "                    writer.add_summary(summary_str, epoch*max_images + ptr)\n",
        "                    \n",
        "                    self.num_fake_inputs+=1\n",
        "            \n",
        "                        \n",
        "\n",
        "                sess.run(tf.assign(self.global_step, epoch + 1))\n",
        "\n",
        "            writer.add_graph(sess.graph)\n",
        "\n",
        "    def test(self):\n",
        "\n",
        "\n",
        "        ''' Testing Function'''\n",
        "\n",
        "        print(\"Testing the results\")\n",
        "\n",
        "        self.input_setup()\n",
        "\n",
        "        self.model_setup()\n",
        "        saver = tf.train.Saver()\n",
        "        init = tf.global_variables_initializer()\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "\n",
        "            sess.run(init)\n",
        "\n",
        "            self.input_read(sess)\n",
        "\n",
        "            chkpt_fname = tf.train.latest_checkpoint(check_dir)\n",
        "            saver.restore(sess, chkpt_fname)\n",
        "\n",
        "            if not os.path.exists(\"./output/imgs/test/\"):\n",
        "                os.makedirs(\"./output/imgs/test/\")            \n",
        "\n",
        "            for i in range(0,100):\n",
        "                fake_A_temp, fake_B_temp = sess.run([self.fake_A, self.fake_B],feed_dict={self.input_A:self.A_input[i], self.input_B:self.B_input[i]})\n",
        "                imsave(\"./output/imgs/test/fakeB_\"+str(i)+\".jpg\",((fake_A_temp[0]+1)*127.5).astype(np.uint8))\n",
        "                imsave(\"./output/imgs/test/fakeA_\"+str(i)+\".jpg\",((fake_B_temp[0]+1)*127.5).astype(np.uint8))\n",
        "                imsave(\"./output/imgs/test/inputA_\"+str(i)+\".jpg\",((self.A_input[i][0]+1)*127.5).astype(np.uint8))\n",
        "                imsave(\"./output/imgs/test/inputB_\"+str(i)+\".jpg\",((self.B_input[i][0]+1)*127.5).astype(np.uint8))\n",
        "\n",
        "\n",
        "def main():\n",
        "    \n",
        "    model = CycleGAN()\n",
        "    if to_train:\n",
        "        model.train()\n",
        "    elif to_test:\n",
        "        model.test()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5OEsjXtigkc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "65ddc911-c417-4f64-e21a-093ac0d9831c"
      },
      "source": [
        "model = CycleGAN()\n",
        "model.input_setup()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-cf1ab0cbcf28>:16: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:276: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:197: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From <ipython-input-7-cf1ab0cbcf28>:19: WholeFileReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.map(tf.read_file)`.\n",
            "image_file_A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKRuujZs2kW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LA0Vl1huCG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls ./input/horse2zebra/trainB/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT5Ubx2xu43L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls  input/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rvp7BS1u9Un",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "7d9d407c-0a48-44a5-c350-c2c7c1e55738"
      },
      "source": [
        "git https://github.com/leehomyc/cyclegan-1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-349d5eddaa91>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    git https://github.com/leehomyc/cyclegan-1\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}